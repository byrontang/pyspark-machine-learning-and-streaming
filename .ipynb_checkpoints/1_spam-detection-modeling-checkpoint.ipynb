{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection\n",
    "\n",
    "## Outline:\n",
    "- Data Preprocessing\n",
    "- Modeling\n",
    "    - Naive Bayes\n",
    "    - Naive Bayes + ngram\n",
    "    - Logistic Regression\n",
    "    - Random Forest\n",
    "- Best Model\n",
    "    - Naive Bayes Assumptions and Algorithm\n",
    "- Next Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, StringIndexer, NGram\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|spam|             message|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "+----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = spark.read.option(\"delimiter\",\n",
    "                        \"\\t\").csv('..\\pyspark-machine-learning-and-streaming\\data\\SMSSpamCollection').toDF('spam', 'message')\n",
    "raw.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to tran and test sets\n",
    "trainingData, testData = raw.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word\n",
    "tokenizer = Tokenizer().setInputCol('message').setOutputCol('words')\n",
    "\n",
    "# Custom stopwords\n",
    "stopwords = StopWordsRemover().getStopWords() + ['-']\n",
    "\n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol('words').setOutputCol('filtered')\n",
    "\n",
    "# Set 2-gram\n",
    "bigram = NGram().setN(2).setInputCol('filtered').setOutputCol('bigrams')\n",
    "\n",
    "# Generate features\n",
    "cvmodel = CountVectorizer().setInputCol('filtered').setOutputCol('features')\n",
    "cvmodel_ngram = CountVectorizer().setInputCol('bigrams').setOutputCol('features')\n",
    "\n",
    "# Convert to binary label\n",
    "indexer = StringIndexer().setInputCol('spam').setOutputCol('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|spam|             message|               words|            filtered|             bigrams|            features|label|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|[go jurong, juron...|(13459,[8,12,33,6...|  0.0|\n",
      "| ham|Ok lar... Joking ...|[ok, lar..., joki...|[ok, lar..., joki...|[ok lar..., lar.....|(13459,[0,26,307,...|  0.0|\n",
      "|spam|Free entry in 2 a...|[free, entry, in,...|[free, entry, 2, ...|[free entry, entr...|(13459,[2,14,20,3...|  1.0|\n",
      "| ham|U dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|[u dun, dun say, ...|(13459,[0,71,83,1...|  0.0|\n",
      "| ham|Nah I don't think...|[nah, i, don't, t...|[nah, don't, thin...|[nah don't, don't...|(13459,[36,39,141...|  0.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_proprocess = Pipeline(stages = [tokenizer, remover, bigram, cvmodel, indexer])\n",
    "preprocessed = pipeline_proprocess.fit(raw)\n",
    "preprocessed.transform(raw).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Three models, Naive Bayes, Logistic Regression, and Random Forest, are used for modeling. For Naive Bayes, pipelines with and without ngram are used. For the other two models, only piple without ngram is used.\n",
    "\n",
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|             message|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "| came to look at ...|  0.0|[-121.16373514872...|[0.99999999991817...|       0.0|\n",
      "| gonna let me kno...|  0.0|[-92.465416881813...|[0.99999999999987...|       0.0|\n",
      "| said kiss, kiss,...|  0.0|[-104.31458352698...|[0.99999999992619...|       0.0|\n",
      "|&lt;#&gt;  is fas...|  0.0|[-304.19496805245...|[1.0,3.4528742938...|       0.0|\n",
      "|'An Amazing Quote...|  0.0|[-87.212993301199...|[0.99997998123937...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "0.9342623156311369\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(smoothing=1)\n",
    "pipeline = Pipeline(stages = [tokenizer, remover, cvmodel, indexer, nb])\n",
    "model = pipeline.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "predictions.select('message', 'label', 'rawPrediction', 'probability', 'prediction').show(5)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol('label').setRawPredictionCol('prediction').setMetricName('areaUnderROC')\n",
    "AUC = evaluator.evaluate(predictions)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes could generate fairly good prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes + ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8593155893536122\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(smoothing=1)\n",
    "pipeline = Pipeline(stages = [tokenizer, remover, bigram, cvmodel_ngram, indexer, nb])\n",
    "model = pipeline.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "#predictions.select('message', 'label', 'rawPrediction', 'probability', 'prediction').show(5)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol('label').setRawPredictionCol('prediction').setMetricName('areaUnderROC')\n",
    "AUC = evaluator.evaluate(predictions)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suprisingly, including ngram does not improve prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "pipeline = Pipeline(stages = [tokenizer, remover, cvmodel, indexer, log_reg])\n",
    "model = pipeline.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol('label').setRawPredictionCol('prediction').setMetricName('areaUnderROC')\n",
    "AUC = evaluator.evaluate(predictions)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of Logistic Regression shows no better performance than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5114068441064639\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier().setLabelCol('label').setFeaturesCol('features').setNumTrees(10)\n",
    "pipeline = Pipeline(stages = [tokenizer, remover, cvmodel, indexer, rf])\n",
    "model = pipeline.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol('label').setRawPredictionCol('prediction').setMetricName(\"areaUnderROC\")\n",
    "AUC = evaluator.evaluate(predictions)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither does Random Forest generate much better result than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "Naive Bayes without ngram in pipeline is clearly the best model for spam detection. The natural of this model actually fit very well to the problem, where the data fram contains sparse data. On the other hand, logistic regression and random forest do not have any advantages when it comes to this type of question/dataset. A brief introduction on Naive Bayes model is discussed below.\n",
    "\n",
    "### Naive Bayes Assumption & Algorithm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "With the best model identified, the next step would be to build an application that predicts the spam message with the model on the steaming data. The application will connect to flume to retrieve streaming data and make prediction in near real time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
