{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|spam|             message|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "+----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = spark.read.option(\"delimiter\",\"\\t\").csv('..\\\\case study 1 dataset\\\\SMSSpamCollection').toDF('spam', 'message')\n",
    "raw.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|spam|             message|               words|\n",
      "+----+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract word\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer().setInputCol('message').setOutputCol('words')\n",
    "transformed = tokenizer.transform(raw)\n",
    "transformed.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+\n",
      "|spam|             message|               words|            filtered|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover().setInputCol('words').setOutputCol('filtered')\n",
    "cleaned = remover.transform(transformed)\n",
    "cleaned.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stopwords\n",
    "stopwords = StopWordsRemover().getStopWords() + ['-']\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol('words').setOutputCol('filtered')\n",
    "cleaned = remover.transform(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "cvmodel = CountVectorizer().setInputCol('filtered').setOutputCol('features').fit(cleaned)\n",
    "featured = cvmodel.transform(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to binary label\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer().setInputCol('spam').setOutputCol('label').fit(featured)\n",
    "indexed = indexer.transform(featured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to tran and test sets\n",
    "training, test = indexed.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(13459,[3,7,44,21...|  0.0|       0.0|\n",
      "|(13459,[3,87,117,...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "log_reg = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "lrModel = log_reg.fit(training)\n",
    "predictions = lrModel.transform(test)\n",
    "predictions.select('features', 'label', 'prediction').show(2)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol('label').setRawPredictionCol('prediction').setMetricName('areaUnderROC')\n",
    "AUC = evaluator.evaluate(predictions)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5040983606557377\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier().setLabelCol('label').setFeaturesCol('features').setNumTrees(10)\n",
    "model = rf.fit(training)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol('label').setRawPredictionCol('prediction').setMetricName(\"areaUnderROC\")\n",
    "AUC = evaluator.evaluate(predictions)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|bigrams                                                                                                                                                                                      |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[go jurong, jurong point,, point, crazy.., crazy.. available, available bugis, bugis n, n great, great world, world la, la e, e buffet..., buffet... cine, cine got, got amore, amore wat...]|\n",
      "|[ok lar..., lar... joking, joking wif, wif u, u oni...]                                                                                                                                      |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|spam|             message|               words|            filtered|             bigrams|            features|label|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|[go jurong, juron...|(37497,[8268,8605...|  0.0|\n",
      "| ham|Ok lar... Joking ...|[ok, lar..., joki...|[ok, lar..., joki...|[ok lar..., lar.....|(37497,[8295,1483...|  0.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "0.5\n",
      "Wall time: 9.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Introduce bi-gram and note the change in accuracy\n",
    "from pyspark.ml.feature import NGram\n",
    "bigram = NGram().setN(2).setInputCol('filtered').setOutputCol('bigrams')\n",
    "bigramDataFrame = bigram.transform(cleaned)\n",
    "bigramDataFrame.select('bigrams').show(2, False)\n",
    "\n",
    "cvmodel = CountVectorizer().setInputCol('bigrams').setOutputCol('features').fit(bigramDataFrame)\n",
    "featured = cvmodel.transform(bigramDataFrame)\n",
    "indexed = indexer.transform(featured)\n",
    "indexed.show(2)\n",
    "\n",
    "training, test = indexed.randomSplit([0.7, 0.3])\n",
    "lrModel = log_reg.fit(training)\n",
    "predictions = lrModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol('label').setRawPredictionCol('prediction').setMetricName(\"areaUnderROC\")\n",
    "print(evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems bi-gram doesn't help prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5025641025641026\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Decide on a strategy and generate a data pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "tokenizer = Tokenizer().setInputCol('message').setOutputCol('words')\n",
    "stopwords = StopWordsRemover().getStopWords() + ['-']\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol('words').setOutputCol('filtered')\n",
    "cvmodel = CountVectorizer().setInputCol('filtered').setOutputCol('features').fit(cleaned)\n",
    "indexer = StringIndexer().setInputCol('spam').setOutputCol('label').fit(featured)\n",
    "\n",
    "rf = RandomForestClassifier().setLabelCol('label').setFeaturesCol('features').setNumTrees(10)\n",
    "pipeline = Pipeline().setStages([tokenizer, remover, cvmodel, indexer, rf])\n",
    "\n",
    "training, test = raw.randomSplit([0.7, 0.3])\n",
    "model = pipeline.fit(training)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol('label').setRawPredictionCol('prediction').setMetricName(\"areaUnderROC\")\n",
    "AUC = evaluator.evaluate(predictions)\n",
    "print(AUC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
